{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPzavAeNWpcv"
   },
   "outputs": [],
   "source": [
    "pip install transformers torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "rme4xu6msCVd",
    "outputId": "a6366f13-1f37-4c38-a254-c9470dacfacf"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload the file\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Display the uploaded file(s)\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"Uploaded file: {filename}\")\n",
    "    file_path = filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mdWB6InosFbY",
    "outputId": "bc57b508-afb6-4e80-97ad-5644bacf9f85"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torchaudio\n",
    "\n",
    "# Load Whisper processor and large-v2 model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v2\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v2\")\n",
    "\n",
    "# Ensure the model is on the appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Function to preprocess MP3 audio file\n",
    "def preprocess_audio(file_path, sampling_rate=16000):\n",
    "    # Load the audio file\n",
    "    audio, sr = torchaudio.load(file_path)\n",
    "    # Resample to 16kHz if necessary\n",
    "    if sr != sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sampling_rate)\n",
    "        audio = resampler(audio)\n",
    "    return audio.squeeze(0)  # Remove channel dimension if stereo\n",
    "\n",
    "# Function to split long audio into chunks\n",
    "def split_audio(audio, chunk_duration, sampling_rate):\n",
    "    num_samples_per_chunk = chunk_duration * sampling_rate\n",
    "    return [audio[i:i + num_samples_per_chunk] for i in range(0, len(audio), num_samples_per_chunk)]\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess the audio\n",
    "audio_tensor = preprocess_audio(file_path)\n",
    "\n",
    "# Split the audio into smaller chunks (30 seconds each)\n",
    "chunk_duration = 30  # Duration of each chunk in seconds\n",
    "chunks = split_audio(audio_tensor, chunk_duration, sampling_rate=16000)\n",
    "\n",
    "# Transcribe each chunk and concatenate results\n",
    "transcriptions = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    input_features = processor(chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "    forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"ta\", task=\"translate\")\n",
    "    generated_ids = model.generate(\n",
    "        input_features,\n",
    "        forced_decoder_ids=forced_decoder_ids,\n",
    "        max_new_tokens=444,  # Allow enough tokens for longer transcription\n",
    "    )\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    transcriptions.append(transcription)\n",
    "    print(f\"Chunk {i + 1}/{len(chunks)} transcribed.\")\n",
    "\n",
    "# Combine all chunks into the final transcription\n",
    "full_transcription = \" \".join(transcriptions)\n",
    "print(\"\\nFull Transcription:\\n\", full_transcription)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
